EDUC821: Advanced Validity Theory and Test Validation <PROF_FULL_NAME >, Ph.D. N110 Furcolo Hall <PHONE> (phone) <PROF_EMAIL> Office Hours for Spring 2022: Wednesdays: 1:00-4:00 Other times by appointment Course Objectives Validity has been described as the most important aspect in educational testing, yet many people working in educational measurement have trouble articulating what validity is, and even more trouble validating the use of a test for a particular purpose. The purpose of this course is to introduce you to different perspectives and theories of test validity and to the process of accumulating validity evidence for educational tests. We will address validation issues in educational testing, employment testing, certification testing, and other areas. The course is primarily based on the Standards for Educational and Psychological Testing (AERA, APA, & NCME, 2014; 2018) and Messick (1989), but we will also review and discuss many other seminal readings in the validity literature. Upon successful completion of this course, you will have a firm grasp of the technical and philosophical aspects of test validity and will have the skills to initiate and carry out a validity agenda for an educational testing program. Topics to be covered in this course include: • Origins and evolution of validity theory • The AERA, APA, & NCME Standards for Educational and Psychological Testing • Sources of validity evidence • Responsibilities of test developers and test users • Unitary conceptualization of validity • Social considerations, fairness, and equity issues in testing • Statistical methods for validating test scores and evaluating tests • Accumulating evidence in support of a validity argument • Assessing special populations—access and equity issues • Legal versus psychometric criteria for evaluating tests • Validity issues in employment testing • Validating a theory of action Course requirements It is expected you will attend and actively participate in all classes. The reading load for this course is relatively heavy. I expect you to come to class prepared to discuss the extremely interesting reading assignments for that day. In addition, there will be weekly assignments, a midterm assignment, and a final assignment. Grading: The course requirements are given a weight to determine your final grade as follows: Activity  % of Grade  Attendance/Participation  20%  Midterm  10%  Weekly Assignments  35%  Final project  35%  Attendance/participation and all assignments are graded on a 0-100 scale. Each missed class reduces the attendance/participation grade by 10 points. Medical illness and other acceptable emergencies will be exceptions to this policy. Final grades of 94-100 receive an A, 90-93 receive an A-, 87-89 receive a B+, 81-86 receive a B, 78-80 receive a C+, 70-77 receive a C, and below 70 receive an F. The requirements for the final project will be described in class. You will work with me (Professor Sireci) in selecting a topic for the final project, which I need to approve by November 14, 2022. Late assignments: Late assignments will be reduced by one-letter grade for each day late (e.g., a maximum grade of “C” will be given to an exceptional assignment submitted two days late). Unforeseen emergencies, as determined by the professor, will be exceptions to this policy. Textbook The only “textbook” required for this class is the Standards for Educational and Psychological Testing (American Educational Research Association, American Psychological Association, & National Council on Measurement in Education, 2014), which is available for free at https://www.testingstandards.net/open-access-files.html. Both English and Spanish versions are available. We will be using the English version. We will use this resource throughout the course. In addition to this text, I will distribute articles and other publications each week. I suggest you assemble these publications into a binder or electronic folder for the course. Resources for learning course material You have at least four resources for helping understand the material presented in this course. 1) Me: I will do my best to present material clearly in class. Your class notes should be useful for completing assignments and the final project. In addition, I am available outside of class during my office hours and by appointment. You can also ask me questions using e-mail. See the top of this syllabus for office hours and e-mail address. 2) The reading assignments: I selected these assignments because I think they are exceptional for understanding the material taught in the course and represent significant contributions to the validity literature. The only exceptions are the articles I authored. I stuck those in there just to impress you and because it helps my ego to force others to read them. 3) The handouts: I will give you numerous handouts throughout the semester. These handouts are designed to summarize and supplement the lectures. I strongly recommend you review them in completing assignments and exams. 4) Each other: I encourage you to discuss class content and reading assignments with your classmates. Illuminating class discussion is a critical feature of this course. Plagiarism policy: It is expected that you will speak with others about course content and even work collaboratively on some class assignments. However, direct copying of someone else’s work is not allowed. Printing out someone else’s computer output, and handing it in as your own work, is also not allowed. Passing off someone else’s work as your own will result in failing this course. In the University’s Academic Regulations, plagiarism is defined as “knowingly representing the words or ideas of another as one's own work in any academic exercise. This includes submitting without citation, in whole or in part, prewritten term papers of another or the research of another, including but not limited to commercial vendors who sell or distribute such materials.” See http://www.umass.edu/registrar/sites/default/files/academicregs.pdf. The most common plagiarism I see is when students excerpt text from a reading, but do not properly cite where the excerpt came from. So, please make sure you cite the source for any text you do not create yourself. Please see me if you have questions about this policy, or if you have trouble completing any assignments. Accommodation policy: I strive to provide an equal educational opportunity for all students. If you have a physical, psychological, or learning disability, you may be eligible for reasonable academic accommodations to help you succeed in this course. If you have a documented disability that requires an accommodation, please notify me within the first two weeks of the semester so that I can make appropriate arrangements to provide any needed accommodations. If you do not have a documented disability, but have a request to make the course more manageable for you, please let me know. I am happy to provide any supports that may be helpful, and I am happy to discuss alternative ways for you to complete the assignments. Bibliography (Required reading assignments are indicated by *) ACT (2018). WorkKeys applied math technical manual. Iowa City, IA: Author. ACT (2018). WorkKeys graphic literacy technical manual. Iowa City, IA: Author. ACT (2018). WorkKeys workplace documents technical manual. Iowa City, IA: Author. ACT (2000). Content validity evidence in support of ACT’s educational achievement tests: ACT’s 1998-1999 national curriculum study. Iowa City, IA: Author. Aiken, L. R. (1980). Content validity and reliability of single items or questionnaires. Educational and Psychological Measurement, 40, 955-959. Almond, R.G., Steinberg, L.S., & Mislevy, R.J. (2002). Enhancing the design and delivery of assessment systems: A four-process architecture. Journal of Technology, Language, and Assessment 1(5). Available from http://www.jtla.org. American Educational Research Association (2000, July). AERA Position Statement: High-stakes testing in preK-12 education. Downloaded January 31, 2005 from http://www.aera.net/policyandprograms/?id=378. American Educational Research Association, American Psychological Association, & National Council on Measurement in Education. (1999). Standards for educational and psychological testing. Washington, D.C.: American Educational Research Association. American Educational Research Association, American Psychological Association, & National Council on Measurement in Education. (2014). Standards for educational and psychological testing. Washington, D.C.: American Educational Research Association. American Educational Research Association, American Psychological Association y National Council on Measurement in Education. (2018). Estándares para pruebas educativas y psicológicas (M. Lieve, Trans.). American Educational Research Association. American Psychological Association, Committee on Test Standards. (1952). Technical recommendations for psychological tests and diagnostic techniques: A preliminary proposal. American Psychologist, 7, 461-465. American Psychological Association. (1954). Technical recommendations for psychological tests and diagnostic techniques. Psychological Bulletin, 51, (2, supplement). American Psychological Association. (1966). Standards for educational and psychological tests and manuals. Washington, D.C.: Author. American Psychological Association (2000). Report of the task force on test user qualifications. Washington, DC: Author. American Psychological Association (2021, October). Apology to people of color for APA’s role in promoting, perpetuating, and failing to challenge racism, racial discrimination, and human hierarchy in U.S.: Resolution adopted by the APA Council of Representatives. Washington, DC: Author. Available at https://www.apa.org/about/policy/racism-apology. American Psychological Association, American Educational Research Association, & National Council on Measurement in Education. (1974). Standards for educational and psychological tests. Washington, D.C.: American Psychological Association. American Psychological Association, American Educational Research Association, & National Council on Measurement in Education. (1985). Standards for educational and psychological testing. Washington, D.C.: American Psychological Association. Anastasi, A. (1950). The concept of validity in the interpretation of test scores. Educational and Psychological Measurement 10, 67-78. Anastasi, A. (1954). Psychological testing. New York: MacMillan. Anastasi, A. (1980). Abilities and the measurement of achievement. New Directions for Testing and Measurement, 5, 1-10. Anastasi, A. (1986). Evolving concepts of test validation. Annual Review of Psychology, 37, 1-15. Angoff, W. H. (1988). Validity: An evolving concept. In H. Wainer & H.I. Braun (Eds.), Test validity (pp. 19-32). Hillsdale, New Jersey: Lawrence Erlbaum. Araneda, S., Lee, D., Lewis, J., Sireci. S. G., Moon, J. A., Lehman, B., Arslan, B., & Keehner, M. (2022). Exploring relationships among test takers’ behaviors and performance using response process data. Education Sciences, 12, 104. https://doi.org/10.3390/educsci12020104. Arce, A., & Young, M. (2022). Test efficacy: Refocusing validation from college exams to candidates. International Journal of Testing, 22, 100-119. https://doi.org/10.1080/15305058.2021.2019752 Arias, A., & Sireci, S. G. (2021). Validez y validación para pruebas educativas y psicológicas. Revista Iberoamericana de Psicología 14 (1), 11-22. Available at https://reviberopsicologia.ibero.edu.co/article/view/rip.14102/1674. Behizadeh, N. & Engelhard, G. (2015) Involving diverse communities of practice to minimize unintended consequences of test-based accountability systems. Measurement: Interdisciplinary Research and Perspectives, 13, 26-30, DOI: 10.1080/15366367.2015.1016320 Bennett, R. E. (2010). Cognitively based assessment of, for, and as learning (CBAL): A preliminary theory of action for summative and formative assessment. Measurement (8), 70-91. Bennett, R. E., Dean, P., & van Rijn, P. W. (2016). From cognitive-domain theory to assessment practice. Educational Psychologist. DOI: 10.1080/00461520.2016.1141683. Bennett, R. E., & Ward, W. C. (Eds.) (1993). Construction versus choice in cognitive measurement. Hillsdale, NJ: Erlbaum. Berman, A., Haertel, E. H., & Pellegrino, J. W. (Eds.) (2020). Comparability Issues in Large-Scale Assessment: Issues and recommendations. Washington, DC: National Academy of Education Press. *Bhola, D. S., Impara, J. C., & Buckendahl, C. W. (2003). Aligning tests with states' content standards: Methods and issues. Educational Measurement: Issues and Practice, 22(3), 21-29. Binet, A. (1905). New methods for the diagnosis of the intellectual level of subnormals. L'Année Psychologique, 12, 191-244. Binet, A., & Henri, B. (1899). La psychologic individuelle. Amiee Psychol., 2, 411-465. Bingham, W.V. (1937). Aptitudes and aptitude testing. New York: Harper. Borsboom, D., Cramer, A. O., Kievit, R. A., Scholten, A. Z., & Frani., S. (2009). The end of construct validity. In The concept of validity: Revisions, new directions and applications. IAP Information Age Publishing. Braun, H. I., Jackson, D. N., & Wiley, D. E. (Eds.) (2002). The role of constructs in psychological and educational measurement. Mahwah, NJ: Erlbaum. Brennan, R. L. (1998). Misconceptions at the intersection of measurement theory and practice. Educational Measurement: Issues and Practice, 17(1), 5-9, 30. Bridgeman, B. (2009). A note on presenting what predictive validity numbers mean. Applied Measurement in Education, 22, 109-119. Bridgeman, B., & cho, Y. (2016). Predicting grades from an English language assessment: The importance of peeling the onion. Briggs, D. C. (2012, April). Making inferences about growth and value-added: Design issues for the PARCC consortium. Paper presented at the annual meeting of the National Council on Measurement in Education, Vancouver, BC. *Byrne, B. M., & van der Vijver, F. J. R. (2010). Testing for measurement and structural equivalence in large-scale cross-cultural studies: Addressing the issue of nonequivalence. International Journal of Testing, 10, 107-132. California Community Colleges (2001). Standards, policies, and procedures for the evaluation of assessment instruments used in the California community colleges (4th edition). Camara, W. J. (2013). Defining and measuring college and career readiness: A validation framework. Educational Measurement: Issues and Practice, 32(4), 16-27. Camilli, G., & Shepard, L. A. (1994). Methods for identifying biased test items. Thousand Oaks, CA: Sage. *Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the multitrait.multimethod matrix. Psychological Bulletin, 56, 81-105. Cascio, W.F., Outtz, J., Zedeck, S., & Goldstein, I.L. (1991). Statistical implications of six methods of test score use in personnel selection. Human Performance, 4, 233-264. Chapelle, C. A., Enright, M. K., & Jamieson, J. (2010). Does an argument.based approach to validity make a difference?. Educational measurement: Issues and practice, 29(1), 3-13. Crocker, L. (2003). Teaching for the test: Validity, fairness, and moral action. Educational Measurement: Issues and Practice, 22(3), 5-11. Crocker, L. M., Miller, D., and Franks E. A. (1989). Quantitative methods for assessing the fit between test and curriculum. Applied Measurement in Education, 2, 179-194. Cronbach, L. J. (1971). Test Validation. In R.L. Thorndike (Ed.) Educational measurement (2nd ed., pp. 443-507). Washington, D.C.: American Council on Education. Cronbach, L. J. (1988). Five perspectives on the validity argument. In H. Wainer & H.I. Braun (Eds.), Test validity (pp. 3-17). Hillsdale, New Jersey: Lawrence Erlbaum. *Cronbach, L. J. & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52, 281-302. Cureton, E.E. (1951). Validity. In E. F. Lindquist (Ed.), Educational measurement (1st ed., pp. 621.694). D’Agostino, J. V., & Bonner, S. M. (2009). High school exit exam scores and university performance. Educational Assessment, 14, 25-47. D’Agostino, J. V., Welsh, M., & Corson, N. M. (2007). Instructional sensitivity of a state’s standards-based assessment. Educational Assessment, 12, 1-22. Daro, P., Huges, G. B., & Stancavage, F. (2016). Study of the alignment of the 2015 NAEP mathematics items at grades 4 and 8 to the Common Core State Standards for mathematics. Washington, DC: American Institutes for Research. Davis-Becker, S. L., & Buckendahl, C. W. (2013). A proposed framework for evaluating alignment studies. Educational Measurement: Issues and Practice, 32(1), 23-33. Dee, T. S., & Jacob. B. (2011). The impact of No Child Left Behind on student achievement. Journal of Policy Analysis and Management, 30, 418-446. http://www.jstor.org/stable/23018959. Dumas, D., Dong, Y., & McNeish, D. (in press). ‘How fair is my test’: A ratio statistic to help represent consequential validity. European Journal of Psychological Assessment. Dixon-Román, E. (2020). A haunting logic of psychometrics: Toward the speculative and indeterminacy of blackness in measurement. Educational Measurement: Issues and Practice, 39(3), 94-96. Dixon-Román, E., Everson, H. T., & McArdle, J. J. (2013). Race, poverty and SAT scores: Modeling the influences of family income on black and white high school students’ SAT performance. Teachers College Record, 115, 1-33. Doorey, N., & Polikoff, M. (2015). Evaluation of the Massachusetts Comprehensive Assessment System and the Partnership for Assessment of Readiness for College and Careers. Fordham Institute. Dorans, N. J., & Middleton, K. (2012). Addressing the extreme assumptions of presumed linkings. Journal of Educational Measurement, 49, 1-18. Dorans, N.J. & Lawrence, I. M. (1987). The internal construct validity of the SAT. (Research Report). Princeton, NJ: Educational Testing Service. Ebel, R. L. (1956). Obtaining and reporting evidence for content validity. Educational and Psychological Measurement, 16, 269-282. *Ebel, R.L. (1961). Must all tests be valid? American Psychologist, 16, 640-647. Ebel, R.L. (1977). Comments on some problems of employment testing. Personnel Psychology, 30, 55-63. Embretson (Whitley), S. (1983). Construct validity: construct representation versus nomothetic span. Psychological Bulletin, 93, 179-197. Evers, A. (2012). The internationalization of test reviewing: trends, differences, and results. International Journal of Testing, 12, 136– 156. Faulkner-Bond, M. F., & Sireci, S. G. (2015). Validity issues in assessing linguistic minorities. International Journal of Testing, 15, 114-135. Faulkner-Bond, M., & Soland (2020). Comparability when assessing English learner students. In A. Berman, E. Haertel, & J. Pellegrino (Eds.) Comparability issues in large-scale assessment (pp. 149– 175). Washington, DC: National Academy of Education Press. Ford, D. Y., & Helms, J. E. (2012). Overview and Introduction: Testing and Assessing African Americans: “Unbiased” Tests are still unfair. The Journal of Negro Education, 81(3), 186-189. Frederiksen, N., Mislevy, R. J., & Bejar, I. I. (Eds.) (1993). Test theory for a new generation of tests. Hillsdale, NJ: Erlbaum. Geisinger, K. F. (1992). The metamorphosis in test validity. Educational Psychologist, 27, 197-222. Geisinger, K. F. (1994). Psychometric issues in testing students with disabilities. Applied Measurement in Education, 7, 121-140. Geisinger, K. F. (2000). Psychological testing at the end of the millennium: A brief historical review. Professional Psychology: Research and Practice, 31, 117-118. Geisinger, K. F. (2005). The testing industry, ethnic minorities, and those with disabilities. (2005). In R. Phelps (Ed.), Defending standardized testing (pp. 187-203). Mahwah, NJ: Erlbaum. Georgia Department of Education and Data Recognition Corporation (2019). Georgia Milestones Assessment System 2019 operational technical report. Atlanta: Georgia Department of Education. Gierl, M.J., Leighton, J.P., & Hunka, S.M. (2000). Exploring the logic of Tatsuoka’s rule-space model for test development and analysis. Educational Measurement: Issues and Practice, 19(3), 34.44. Gökçe, S., Berberoglu, G., Sireci, S.G., & Wells, C. S. (2021). Linguistic distance and translation DIF on TIMSS mathematics assessment items. Journal of Psychoeducational Assessment. https://doi.org/10.1177/07342829211010537. Goodenough, F. L. (1949). Mental testing. New York: Rinehart. Gould, S. J. (1996). The mismeasure of man. New York: Norton & Son. Green, P., & Sireci, S.G. (1999). Legal and psychometric issues in testing students with disabilities. Journal of Special Education Leadership. *Guilford, J.P. (1946). New standards for test evaluation. Educational and Psychological Measurement, 6, 427-439. Guion, R. M. (1977). Content validity: the source of my discontent. Applied Psychological Measurement, 1, 1-10. Guion, R. M. (1978). Scoring of content domain samples: the problem of fairness. Journal of Applied Psychology, 63, 499-506. Guion, R. M. (1980). On trinitarian doctrines of validity. Professional Psychology, 11, 385-398. *Gulliksen, H. (1950a). Intrinsic validity. American Psychologist, 5, 511-517. Gulliksen, H. (1950b). Theory of mental tests. New York: Wiley. Haertel, E. H. (1999). Validity arguments for high-stakes testing: In search of the evidence. Educational Measurement: Issues and Practice, 18(4), 5-9. *Haertel, E. (2013). Getting the help we need. Journal of Educational Measurement, 50, 84-90. Haladyna, T. M., & Downing, S. M. (2004). Construct-irrelevant variance in high-stakes testing. Educational Measurement: Issues and Practice, 23(1), 17-27. Hambleton, R. K. (1980). Test score validity and standard setting methods. In R.A. Berk (ed.), Criterion-referenced measurement: the state of the art. Baltimore: Johns Hopkins University Press. Hambleton, R. K., (1984). Validating the test score In R.A. Berk (Ed.), A guide to criterion-referenced test construction. Baltimore: Johns Hopkins University Press, pp. 199-230. Haynes, S. N., Richard, D. C. S., & Kubany, E. S. (1995). Content validity in psychological assessment: A functional approach to concepts and methods. Psychological Assessment, 7, 238-247. Helms, J. (2006). Fairness is not validity or cultural bias in racial-group assessment: A quantitative perspective. American Psychologist, 61(8), 845-859. Holland, P. W., & Wainer, H. (Eds.). (1993). Differential item functioning. Hillsdale, New Jersey: Lawrence Erlbaum. Hoeve, K. B. (2022). A validity framework for accountability: Educational measurement and language testing. Language Testing in Asia, 12(3), 1-14. https://doi.org/10.1186/s40468-021-00153-2. Hood, S. B. (2009). Validity in psychological testing and scientific realism. Theory & Psychology, 19(4), 451-473. Hubley, A. M., & Zumbo, B. D. (1996). A dialectic on validity: Where we have been and where we are going. The Journal of General Psychology, 123, 207-215. Huff, K. L., & Sireci, S. G. (2001). Validity issues in computer-based testing. Educational Measurement: Issues and Practice, 20 (3), 16-25. International Test Commission (2017). International Test Commission guidelines for translating and adapting tests (2nd Edition). Author. Available for download at http://www.intestcom.org. Jaeger, R. M. (1998). Evaluating the psychometric qualities of the National Board for Professional Teaching Standards’ Assessments: A methodological accounting. Journal of Personnel Evaluation in Education, 12, 189-210. Jarjoura, D. & Brennan, R.L. (1982). A variance components model for measurement procedures associated with a table of specifications. Applied Psychological Measurement, 6, 161-171. Johnson, S. T. (1980). Major issues in measurement today: Their implications for Black Americans. Journal of Negro Education, 49, 253-262. Johnson, S. T. (2000). The live creature and its expectations for the future. Journal of Negro Education, 69, 150-158. Johnson, J. L., Trantham, P., & Usher-Tate, B. J. (2019). An evaluative framework for reviewing fairness standards and practices in educational tests. Educational Measurement: Issues and Practice, 38(3), 6-19. *Jenkins J. G., (1946). Validity for what? Journal of Consulting Psychology, 10, 93-98. Kaestle, C. (2013). Testing policy in the United States: A historical perspective. Princeton, NJ: Gordon Commission on the Future of Educational Assessment. https://www.ets.org/Media/Research/pdf/kaestle_testing_policy_us_historical_perspective.pdf. *Kane, M.T. (1992). An argument-based approach to validity. Psychological Bulletin, 112,527-535. Kane, M. (1994). Validating the performance standards associated with passing scores. Review of Educational Research, 64, 425–461. Kane, M. (2006). Validation. In R. L. Brennan (Ed). Educational measurement (4th edition, pp. 17.64). Washington, DC: American Council on Education/Praeger. Kane, M. (2009). Validating the interpretations and uses of test scores. In R. Lissitz (Ed.), The Concept of Validity: Revisions, New Directions and Applications (pp. 39-64). Charlotte, NC: Information Age Publishing Inc. Kane, M. (2013). Validating the interpretations and uses of test scores. Journal of Educational Measurement, 50, 1-73. Kelley, T.L. (1927). Interpretation of educational measurement. Yonkers-on-Hudson, NY: World Book Co. Kinnear B., Schumacher D.J., Driessen E. W., Varpio, L. (2022). How argumentation theory can inform assessment validity: A critical review. Medical Education, 1.12. doi:10.1111/medu.14882 Kobrin, J., L., Patterson, B. F., Shaw, E. J., Mattern, K. D., & Barbuti, S. M. (2008a). Differential validity and prediction of the SAT. College Board research report no. 2008-4. New York: The College Board. Kobrin, J., L., Patterson, B. F., Shaw, E. J., Mattern, K. D., & Barbuti, S. M. (2008b). Validity of the SAT for predicting first-year college grade point average. College Board research report no. 2008-5. New York: The College Board. Koenig, J.A., Sireci, S.G., & Wiley, A. (1998). Evaluating the predictive validity of MCAT scores across diverse applicant groups. Academic Medicine, 73, 65-76. Koljatic, M., Silva, M., & Sireci, S. G. (2021). College admission tests and social responsibility. Educational Measurement: Issues and Practice. https://doi.org/10.1111/emip.12425. Koljatic, M., Silva, M., & Sireci, S. G. (2021). College admission tests and social responsibility: A response to comments. Educational Measurement: Issues and Practice. Kuncel, N., Campbell, J. P., Ones, D. (1998). Validity of the Graduate Record Examination: Estimated or tacitly known? American Psychologist, 53(5), 567-568. LaDuca, A. (1994). Validation of professional licensure examinations. Evaluation & the Health Professions, 17, 178-197. Lane, S. (2014). Validity evidence based on testing consequences. Psicothema. doi: 10.7334/psicothema2013.258. Lane, S., Parke, C. S., & Stone, C. A. (1998). A framework for evaluating the consequences of assessment programs. Educational Measurement: Issues and Practice, 17(2), 24-28. Lane, S., & Stone, C. A. (2002). Strategies for examining the consequences of assessment and accountability programs. Educational Measurement: Issues and Practice, 21(1), 23-41. Lawshe, C. H. (1975). A quantitative approach to content validity. Personnel Psychology, 28, 563-575. Leighton, J. P. (2004). Avoiding misconception, misuse, and missed opportunities: The collection of verbal reports in educational achievement testing. Educational Measurement: Issues and Practice, 23(4), 6–15. Leighton, J.P. & Gierl, M.J. (Eds.). (2007). Cognitive diagnostic assessment for education: Theories and applications. Cambridge, MA: Cambridge University Press. Leach, M.M. & Oakland, T. (2007). Ethics standards impacting test development and use: A Review of 31 ethics codes impacting practices in 35 countries, International Journal of Testing, 7(1). 71-88. Leadership Conference on Civil and Human Rights. (2015). Participation in assessments critical for expanding educational opportunity for all students [Press release]. Retrieved from http://www.civilrights.org/press/2015/anti-testing-efforts.html *Lehman, N. (1999, September 6). Behind the SAT. Newsweek, 134(10), 52-57. Lehman, N. (1999). The big test. New York: Farrar, Straus, & Giroux. Lennon, R. T. (1956). Assumptions underlying the use of content validity. Educational and Psychological Measurement, 16, 294-304. Lindquist, E. F. (Ed.). (1951). Educational measurement. Washington, D.C.: American Council on Education. Linn, R. L. (1982). Ability testing, individual differences, prediction, and differential prediction. In A. Wigdor & W. Garner (Eds.), Ability testing: Uses, consequences, and controversies (pp. 335.388). Washington, DC: National Academy Press. Linn, R. L. (1984). Selection bias: Multiple meanings. Journal of Educational Measurement, 21, 33.47. Linn, R. L. (Ed.). (1989). Educational measurement, (3rd ed.). Washington, D.C.: American Council on Education. Linn, R.L. (1994). Criterion-referenced measurement: a valuable perspective clouded by surplus meaning. Educational Measurement: Issues and Practice, 13, 12-15. Linn, R. L. (2000). Assessments and accountability. Educational Researcher, 29(2), 4-16. Linn, R. L. (2005). Evaluating the validity of assessments: The consequences of test use. Educational Measurement: Issues and Practice, 16(2), 14-16. *Linn, R. L. (2009). The concept of validity in the context of NCLB. In R. Lissitz (Ed.), The Concept of Validity: Revisions, New Directions and Applications (pp. 195-212). Charlotte, NC: Information Age Publishing Inc. Loevinger, J. (1957). Objective tests as instruments of psychological theory. Psychological Reports, 3, 635-694 (Monograph Supplement 9). Ludlow, L. H. (2001). Teacher test accountability: From Alabama to Massachusetts. Education Policy Analysis Archives, 9(6). Available at http://epaa.asu.edu/epaa/v9n6.html. Lyons, S., Hinds, F. and Poggio, J. (2021), Evolution of equity perspectives on higher education admissions testing: A call for increased critical consciousness. Educational Measurement: Issues and Practice. https://doi.org/10.1111/emip.12458 Maguire, T., Hattie, J., & Haig, B. (1994). Construct validity and achievement assessment. The Alberta Journal of Educational Research, 15, 109-126. Marchant, G. J. & Paulson, S. E. (2005, January 21). The relationship of high school graduation exams to graduation rates and SAT scores. Education Policy Analysis Archives, 13(6). http://epaa.asu.edu/epaa/v13n6/. Martone, A., & Sireci, S. G. (2009). Evaluating alignment between curriculum, assessments, and instruction, Review of Educational Research 4, 1332-1361. *McGinn, D. (1999, September 6). The big score. Newsweek, 134(10), 46-51. Available at http://www.newsweek.com/big-score-166300. McNamara, T. (2006). Validity in language testing: The challenge of Sam Messick’s legacy. Language assessment quarterly, 3, 31-51. Messick, S. (1975). The standard problem: meaning and values in measurement and evaluation. American Psychologist, 30, 955-966. Messick, S. (1980). Test validity and the ethics of assessment. American Psychologist, 35, 1012.1027. Messick, S. (1988). The once and future issues of validity: Assessing the meaning and consequences of measurement. In H. Wainer & H.I. Braun (Eds.), Test validity (pp. 33-45). Hillsdale, New Jersey: Lawrence Erlbaum. Messick, S. (1989a). Meaning and values in test validation: the science and ethics of assessment. Educational Researcher, 18, 5-11. *Messick, S. (1989b). Validity. In R. Linn (Ed.), Educational measurement, (3rd ed., pp. 13-100). Washington, D.C.: American Council on Education. Meyer, G. J., Finn, S. E., Eyde, L. D., Kay, G. G., Moreland, K. L., Dies, R. R., Eisman, E. J., Kubiszyn, T. W., & Reed, G. M. (2001). Psychological testing and psychological assessment: A review of evidence and issues. American Psychologist, 56, 128-165. Millsap, R. E. (2007). Invariance in measurement and prediction revisited. Psychometrika, 72, 461.473. Mislevy, R. J. (2003). Rehabilitating psychometrics: Commentary on Pellegrino and Chudowsky’s “the foundation of assessment.” Measurement: Interdisciplinary Research and perspectives, 1, 162-165. Mislevy, R. J. (2009). Validity from the perspective of model-based reasoning. In R. Lissitz (Ed.), The Concept of Validity: Revisions, New Directions and Applications (pp. 83-108). Charlotte, NC: Information Age Publishing Inc. Mislevy, R. J. (2018). Sociocognitive foundations of educational measurement. Routledge. Mosier, C. I. (1947). A critical examination of the concepts of face validity. Educational and Psychological Measurement, 7, 191-205. National Council on Measurement in Education (2012). Testing and data integrity in the administration of statewide student assessment programs. Madison, WI: Author. Nichols, P. D., Chipman, S. F. & Brennan, R. L. (Eds.) (1995). Cognitively diagnostic assessment. Hillsdale, NJ: Erlbaum. Noble, T., Sireci, S. G., Wells, C. S., Kachchaf, R. R., Rosebery, A. S., & Wang, Y. C. (2020). Targeted linguistic simplification of science test items for English learners. American Educational Research Journal, 57, 2175-2209. Nunnally, J. C. (1967). Psychometric theory. New York: McGraw-Hill. O’Leary, T. M., Hattie, J. A. C., & Griffin, P. (2017). Actual interpretations and use of scores as aspects of validity. Educational Measurement: Issues and Practice, 36(2), 16-23. O’Neil, T., Sireci, S. G., & Huff, K. F. (2004). Evaluating the consistency of test content across two successive administrations of a state-mandated science assessment. Educational Assessment, 9, 129-151. O’Donnell, F., & Sireci, S. G. (2021). Language matters: Teacher and parent perceptions of achievement labels from educational tests. Educational Assessment. DOI: 10.1080/10627197.2021.2016388. Outtz, J. L. (2003). The role of cognitive ability tests in employment selection. Human Performance, 15(1/2), 161–171. Padilla, J. & Benítez, I,(2014). Validity evidence based on response processes. Psyicothema, 26, 136-144. Pearson, K. (1896). Mathematical contributions to the theory of evolution. III. Regression, heredity and panmixia, Philosophical Transactions of the Royal Society A, 187, 253-318. Pedhazur, E. J., & Schmelkin, L. P. (1991). Measurement, design, and analysis: An integrated approach. Hillsdale, NJ: Lawrence Erlbaum. Penfield, R. D., & Miller, J. M. (2004). Improving content validation studies using an asymmetric confidence interval for the mean of expert ratings. Applied Measurement In Education, 17(4), 359-370. Penfield, R. D. (2010). Test-based grade retention: Does it stand up to professional standards for fair and appropriate test use? Educational Researcher, 39, 110-119. Phelps, R. (Ed.), (2005). Defending standardized testing. Mahwah, NJ: Erlbaum. Phelps, R. (Ed.), (2009). Correcting fallacies about educational and psychological testing. Washington, DC: American Psychological Association. Phelps, R. (2012). The effects of testing on student achievement: 1910-2010. International Journal of Testing, 12, 21-43. DOI:10.1080/15305058.2011.602920 Phillips, S. E. (2000). GI Forum v. Texas Education Agency: Psychometric evidence. Applied Measurement in Education, 13, 343-385. *Pitoniak, M. J., Sireci, S. G., & Luecht, R. M. (2002). A multitrait-multimethod validity investigation of scores from a professional licensure exam. Educational and Psychological Measurement, 62, 498-516. Poggio, J. P., Glasnapp, D. R., Miller, M. D., Tollefson, N., & Burry, J.A. (1986, summer). Strategies for validating teacher certification tests. Educational Measurement: Issues and Practice, 5(2), 18-25. Polikoff, M. S. (2010). Instructional sensitivity as a psychometric property of assessments. Educational Measurement: Issues and Practice, 29(4), 3-14. Popham, W. J. (1992). Appropriate expectations for content judgments regarding teacher licensure tests. Applied Measurement in Education, 5, 285-301. Popham, W. J. (1994). The instructional consequences of criterion-referenced clarity. Educational Measurement: Issues and Practice, 13, 15-20,39. Popham, W. J. (1997). Consequential validity: Right concern-wrong concept. Educational Measurement: Issues and Practice, 16(2), 9-13. Popham, W.J., Baker, E.L., Berliner, D.C, Yeakey, C.C., Pelligrino, J.W., Quenemoen, R.F., Roderiquez-Brown, F. V., Sandifer, P.D., Sireci, S.G., & Thurlow, M.L. (2001, October). Building tests to support instruction and accountability: A guide for policymakers. Commission on Instructionally Supportive Assessment. Available at http://www.aasa.org/issues_and_insights/assessment/Building_Tests.pdf. Rabinowitz, S., & Brandt, S. (2001). Computer-based assessment: Can it deliver on its promise? WestEd Knowledge Brief. Downloaded January 30, 2005 from http://www.wested.org/cs/we/view/rs/568. Raju, N. S., Lafitte, L. J., & Byrne, B. M. (2002). Measurement equivalence: A comparison of methods based on confirmatory factor analysis and item response theory. Journal of Applied Psychology, 87, 517-529. Randall, J. (2021). “Color-neutral” is not a thing: Redefining construct definition and representation through a justice-oriented critical antiracist lens. Educational Measurement: Issues and Practice. Randall, J. (2021b), Commentary: From construct to consequences: Extending the notion of social responsibility. Educational Measurement: Issues and Practice. https://doi.org/10.1111/emip.12452 Randall, J., Poe, M., & Slomp, D. (2021). Ain’t oughta be in the dictionary: Getting to justice by dismantling anti-black literacy assessment practices. Journal of Adolescent and Adult Literacy, 64, 594-599. Randall, J., Slomp, D., Poe, M., & Oliveri, M. E. (2022). Disrupting white supremacy in assessment: toward a justice-oriented, antiracist validity framework. Educational Assessment, https://doi.org/10.1080/10627197.2022.2042682. Randall, J., & Garcia, A. (2016). The history of testing special populations. In C. Wells & M. F. Bond (Eds., pp. 373-394). Educational measurement: From foundations to future. Guilford Press. Raymond, M. R. (2001). Job analysis and the specification of content for licensure and certification exams. Applied Measurement in Education, 14, 369-415. Reckase, M. D. (1998). Consequential validity from the test developer's perspective. Educational Measurement: Issues and Practice, 17(2), 13-16. *Rios, J., & Wells, C.S. (2014). Validity evidence based on internal structure. Psicothema, 26(1), 108-116. *Rulon, P. J. (1946). On the validity of educational tests. Harvard Educational Review, 16, 290-296. Sanchez, E. I. (2013). Differential effects of using ACT college readiness assessment scores and high school GPA to predict first-year college GPA among racial/ethnic, gender, and income groups. ACT research report series 2013-4. Iowa City: ACT. Sawyer, R. (1996). Decision theory models for validating course placement tests. Journal of Educational Measurement, 33, 271-290. Schmidt, F. L. (1988). Validity generalization and the future of criterion-related validity. In H. Wainer & H.I. Braun (Eds.), Test validity (pp. 173-189). Hillsdale, New Jersey: Lawrence Erlbaum. Schmidt, F. L., Viswesvaran, C., & Ones, D. (2000). Reliability is not validity and validity is not reliability. Personnel Psychology, 53, 901-912. *Shepard, L. A. (1993). Evaluating test validity. Review of Research in Education, 19, 405-450. Shepard, L. A. (1997). The centrality of test use and consequences for test validity. Educational Measurement: Issues and Practice, 16(2), 5-8, 13. Shepard, L. A. (2003). Intermediate steps to knowing what students know. Measurement: Interdisciplinary Research and perspectives, 1, 171-177. Sinha, R., Oswald, F., Imus, A & Schmitt, N. (2011) Criterion-focused approach to reducing adverse impact in college admissions. Applied Measurement in Education, 24, 137-161, DOI: 10.1080/08957347.2011.554605. Sireci, S. G. (1997a). Dimensionality issues related to the National Assessment of Educational Progress. Commissioned paper by the National Academy of Sciences/National Research Council's Committee on the Evaluation of National and State Assessments of Educational Progress, [Document Number 619]. Washington, DC: National Research Council. Sireci, S. G. (1997b). Problems and issues in linking tests across languages. Educational Measurement: Issues and Practice, 16(1), 12-19. Sireci, S. G. (1998a). Gathering and analyzing content validity data. Educational Assessment, 5, 299.321. *Sireci, S. G. (1998b). The construct of content validity. Social Indicators Research, 45, 83-117. *Sireci, S. G. (2005a). Unlabeling the disabled: A perspective on flagging scores from accommodated test administrations. Educational Researcher, 34(1), 3-12. Sireci, S.G. (2005b). Using bilinguals to evaluate the comparability of different language versions of a test. In R.K. Hambleton, P. Merenda, & C. Spielberger (Eds.) Adapting educational and psychological tests for cross-cultural assessment (pp. 117-138). Hillsdale, NJ: Erlbaum. Sireci, S. G. (2006). Content validity. In N. J. Salkind (Ed.) Encyclopedia of measurement and statistics. Thousand Oaks, CA: Sage. Sireci, S. G. (2007). On test validity theory and test validation. Educational Researcher, 36(8), 477.481. Sireci, S. G. (2008). Are educational tests inherently evil? In D. A. Henningfeld (Ed.). At issue: Standardized testing (pp. 10-16). Detroit: Thompson Gale. Sireci, S. G. (2009a). No more excuses: New research on assessing students with disabilities. Journal of Applied Testing Technology, 10 (2). Available at http://www.testpublishers.org/Documents/Special%20Issue%20article%201%20.pdf. *Sireci, S. G. (2009b). Packing and upacking sources of validity evidence: History repeats itself again. In R. Lissitz (Ed.), The Concept of Validity: Revisions, New Directions and Applications (pp. 19-37). Charlotte, NC: Information Age Publishing Inc. Sireci, S. G. (2011). Evaluating test and survey items for bias across languages and cultures. In D. Matsumoto and F. van de Vijver (Eds.) Cross-cultural research methods in psychology (pp. 216-240). Oxford, UK: Oxford University Press. Sireci, S. G. (2012, December). Smarter Balanced Assessment Consortium: Comprehensive validity agenda. Available at http://www.smarterbalanced.org/assessments/development/additional.technical-documentation/ Sireci, S. G. (2013). Agreeing on validity arguments. Journal of Educational Measurement, 50, 99.104. *Sireci, S. G. (2015). A theory of action for validation. In H. Jiao & R. Lissitz (Eds.). The next generation of testing: Common core standards, Smarter-Balanced, PARCC, and the nationwide testing movement (pp. 251-269). Charlotte: Information Age Publishing Inc. Sireci, S. G. (2016a). Comments on valid (and invalid?) commentaries. Assessment in Education: Principles, Policy & Practice, 23, 319-321. *Sireci, S. G. (2016b). On the validity of useless tests. Assessment in Education: Principles, Policy & Practice, 23, 226-235. Sireci, S. G. (2016, September). Want to understand your child’s test score report? Here’s what to ignore. The Conversation. Published online, available at https://theconversation.com/want-to.understand-your-childs-test-scores-heres-what-to-ignore-62155. Sireci, S. G. (2020a) "De-“Constructing” Test Validation," Chinese/English Journal of Educational Measurement and Evaluation, 1. ...........: Available at: https://www.ce.jeme.org/journal/vol1/iss1/3. Sireci, S. G. (2020b). Standardization and UNDERSTANDardization in Educational Assessment, Educational Measurement: Issues and Practice, 39(3) 100-105. Sireci, S. G. (2021). Valuing educational measurement. Educational Measurement: Issues and Practice, 40(1), 7-16. DOI: 10.1111/emip.12415. Sireci, S. G., Baldwin, P., Martone, A., Zenisky, A. L., Kaira, L., Lam, W., Shea, C. L., Han, K. T., Deng, N., Delton, J., & Hambleton, R. K. (2008). Massachusetts Adult Proficiency Tests technical manual: Version 2. Center for Educational Assessment Research Report No. 677. Amherst, MA: University of Massachusetts, Center for Educational Assessment. Available at http://www.umass.edu/remp/CEA_TechMan.html. Sireci, S. G., & Banda, E., & Wells, C. S. (2018). Promoting valid assessment of students with disabilities and English learners. In Elliott, S. N., Kettler, R. J, Beddow, P. A., & Kurz, A., (Eds.), Handbook of Accessible Instruction and Testing Practices: Issues, Innovations, and Application (pp.231-246). Sage. Sireci, S. G., & Faulkner-Bond (2014). Validity evidence based on test content. Psicothema. doi: 10.7334/psicothema2013.256. Sireci, S. G. & Faulkner-Bond, M. F. (2015). Promoting validity in the assessment of English learners and other linguistic minorities. Review of Research in Education, 39, 215-252. Sireci, S. G. & Gandara, M. F. (2016). Testing in educational and developmental settings. In F. Leong et al. (Eds.). International Test Commission handbook of testing and assessment (pp. 187-202). Oxford University Press. Sireci, S. G. & Geisinger, K. F. (1992). Analyzing test content using cluster analysis and multidimensional scaling. Applied Psychological Measurement, 16, 17-31. Sireci, S. G., & Geisinger K. F. (1995). Using subject matter experts to assess content representation: An MDS analysis. Applied Psychological Measurement, 19, 241-255. *Sireci, S. G., & Geisinger, K. F. (1998). Equity issues in employment testing. In J.H. Sandoval, C. Frisby, K.F. Geisinger, J. Scheuneman, & J. Ramos-Grenier (Eds.). Test interpretation and diversity (pp. 105-140). American Psychological Association: Washington, D.C. Sireci, S.G., & Green, P.C. (2000). Legal and psychometric criteria for evaluating teacher certification tests. Educational Measurement: Issues and Practice, 19(1), 22-31, 34. *Sireci, S. G., Han, K. T., & Wells, C. S. (2008). Methods for evaluating the validity of test scores for English language learners. Educational Assessment, 13, 108-131. Sireci, S. G., Hauger, J. B, Wells, C. S., Shea, C., & Zenisky, A. L. (2009). Evaluation of the standard setting on the 2005 grade 12 National Assessment of Educational Progress mathematics test. Applied Measurement in Education, 22, 339-358. *Sireci, S. G., & Parker, P. (2006). Validity on trial: Psychometric and legal conceptualizations of validity. Educational Measurement: Issues and Practice, 25 (3), 27-34. Sireci, S.G., Patsula, L., & Hambleton, R. K. (2005). Statistical methods for identifying flawed items in the test adaptations process. In R.K. Hambleton, P. Merenda, & C. Spielberger (Eds.). Adapting educational and psychological tests for cross-cultural assessment (pp. 93-115). Hillsdale, NJ: Lawrence Erlbaum. Sireci, S. G., & Randall, J. (2021). Evolving notions of fairness in testing in the United States. In M. Bunch & B. Clauser (Eds.), The history of educational measurement: Key advancements in theory, policy, and practice (pp. 111-135). New York: Routledge Sireci, S. G., Rios, J. A., & Powers, S. (2016). Comparing test scores from tests administered in different languages. In N. Dorans & L. Cook (Eds.) Fairness in educational assessment and measurement (pp. 181-202). New York: Routledge. Sireci, S.G., Robin, F., Meara, K., Rogers, H.J., & Swaminathan, H. (2000). An external evaluation of the 1996 Grade 8 NAEP Science Framework. In N. Raju, J.W. Pellegrino, M.W. Bertenthal, K.J. Mitchell & L.R. Jones (Eds.) Grading the nation’s report card: Research from the evaluation of NAEP (pp. 74-100). Washington, D.C.: National Academy Press. Sireci, S. G., & Rodriguez, G. (2016, December). Evaluating the Alignment of the Medición y Evaluación para la Transformación Académica to the Puerto Rico Core Standards. Northampton, MA: Sireci Psychometric Services. Sireci, S. G., & Rodriguez, G. (2022). Validity in educational testing. In S. Brookhart (Ed.). Routledge Encyclopedia of Education New York: Routledge. https://doi.org/10.4324/9781138609877-REE180-1. Sireci, S.G., Rogers, H.J., Swaminathan, H., Meara, K.,& Robin, F. (2000). Appraising the dimensionality of the 1996 Grade 8 NAEP Science Assessment Data. In N. Raju, J.W. Pellegrino, M.W. Bertenthal, K.J. Mitchell & L.R. Jones (Eds.) Grading the nation’s report card: Research from the evaluation of NAEP (pp. 101-122). Washington, D.C.: National Academy Press. Sireci, S. G., & Soto, A. (2016). Validity and accountability: Test validation for 21st-century educational assessments. In H. Braun (Ed.). Meeting the challenges to measurement in an era of accountability (pp. 149-167). New York: Routledge. Sireci, S. G., & Suárez-Alvárez, J. (2022). Deriving decisions from disrupted data. Educational Measurement: Issues and Practice. Published online February 3: https://onlinelibrary.wiley.com/doi/abs/10.1111/emip.12499. *Sireci, S. G., & Talento-Miller, E. (2006). Evaluating the predictive validity of Graduate Management Admissions Test Scores. Educational and Psychological Measurement, 66, 305.317. Smarter Balanced Assessment Consortium (2010, June 23). Race to the top assessment program application for new grants: Comprehensive assessment systems, CFDA Number: 84.395B. OMB Control Number 1810-0699. Society for Industrial Organizational Psychology (2018). Principles for the validation and use of personnel selection procedures (5th edition). Bowling Green, OH: Author. Spearman, C. (1904). General intelligence: objectively determined and measured. American Journal of Psychology, 15, 201–293. Spearman, C. (1907). Demonstration of formulae for true measurement of correlation. American Journal of Psychology, 18, 161-169. Sternberg, R. J., Wagner, R. K., Williams, W. M., & Horvath, J. A. (1995). Testing common sense. American Psychologist, 50, 912-927. Taleporos, E. (1998). Consequential validity: A practitioner's perspective. Educational Measurement: Issues and Practice, 17(2), 20-23 Tenopyr, M.L. (1977). Content-construct confusion. Personnel Psychology, 30, 47-54. Terman, L. M., & Childs, H.G. (1912). A tentative revision and extension of the Binet-Simon measuring scale of intelligence. Journal of Educational Psychology, 3, 61-74. Terman, L. M. (1924). The mental test as a psychological method. Psychological Review, 31(2), 93.117. doi:10.1037/h0070938 Terman, L. M., Lyman, G. Ordahl, G., Ordahl, L., Galbreath, N., & Talbert, W. (1915). The Stanford revision of the Binet-Simon scale and some results from its application to 1000 non-selected children. Journal of Educational Psychology, 6, 551-562. Thorndike, E. L. (1931). Measurement of intelligence. New York: Bureau of Publishers, Columbia University. Thorndike, R. L. (1949). Personnel selection: Test and measurement techniques. New York: Wiley. Thorndike, R. L. (Ed.). (1971). Educational measurement (2nd ed.). Washington. D.C.: American Council on Education. Thorndike, R. L. (1982). Applied Psychometrics. Boston: Houghton-Mifflin. Thurstone, L. L. (1932). The reliability and validity of tests. Ann Arbor, Michigan: Edwards Brothers. Traynor, A. (2017). Does test item performance increase with test-to-standards Alignment?, Educational Assessment, 22, 171-188. Toops, H. A. (1944). The criterion. Educational and Psychological Measurement, 4, 271-297. Tucker, L. R. (1961). Factor analysis of relevance judgments: an approach to content validity. Paper presented at the Invitational Conference on Testing Problems, Princeton, NJ [Reprinted in A. Anastasi (Ed.) Testing problems in perspective (1966), (pp. 577-586). Washington, D.C.: American Council on Education. Turney, A. H. (1934). The concept of validity in mental and achievement testing. Journal of Educational Psychology, 25(2), 81-95. doi:10.1037/h0072182 U.S. Department of Education (2009). Standards and Assessments Peer Review Guidance: Information and Examples for Meeting Requirements of the No Child Left Behind Act of 2001 (revised with technical edits January 12, 2009). Washington, DC: Author. U.S. Department of Education (2015, September). U. S. Department of Education Peer Review of State Assessment Systems: Non-Regulatory Guidance for States for Meeting Requirements of the Elementary and Secondary Education Act of 1965, as amended). Washington, DC: Author. *U.S. Department of Education (2018, September). A state’s guide to the U.S. Department of Education’s assessment peer review process. Washington, DC: Author. U.S. Equal Employment Opportunity Commission (2010). Fact sheet on employment tests and selection procedures. Washington, DC: Author. Available at https://www.eeoc.gov/policy/docs/factemployment_procedures.html. Vogt, D. S., King, D. W., & King, L. A. (2004). Focus groups in psychological assessment: Enhancing content validity by consulting members of the target population. Psychological Assessment, 16, 231-243. Wainer, H. (1993). Measurement problems. Journal of Educational Measurement, 30, 1-21. Wainer, H., & Braun, H. I. (1988). Test validity. Hillsdale, NJ: Lawrence Erlbaum. Wainer, H., & Sireci, S. G. (2005). Item and test bias. Encyclopedia of social measurement volume 2, 365-371. San Diego: Elsevier. Walker, M.E. (2021), Commentary: Achieving educational equity requires a communal effort. Educational Measurement: Issues and Practice. https://doi.org/10.1111/emip.12465. Webb, N. L. (1999, August). Alignment of science and mathematics standards and assessments in four states. Research Mongraph No. 18. Madison, WI: National Institute for Science Education, University of Wisconsin-Madison. Webb, N. L. (2007). Issues related to judging the alignment of curriculum standards and assessments. Applied Measurement in Education, 20(1), 7-25. Wells, C. S., Baldwin, S., Hambleton, R. K., Sireci, S. G., Karantonis, A. & Jirka, S. (2009). Evaluating score equity assessment for state NAEP. Applied Measurement in Education, 22, 394-408. Wells, C. S., & Sireci, S. G. (2020). Evaluating random and systematic error in student growth percentiles. Applied Measurement in Education. Wednler, C., & Brodgeman, B. (Eds.) (2014). The research foundation for the GRE® revised General Test: A compendium of studies. Princeton, NJ: Educational Testing Service. Whitney, C. R., & Candelaria, C. A. (2017). The effects of No Child Left Behind on children’s socioemotional outcomes. AERA Open. https://doi.org/10.1177/2332858417726324 Willingham, W. W. (1988). Testing handicapped people—the validity issue. In H. Wainer & H. I. Braun (Eds.), Test validity (pp. 89-103). Hillsdale, New Jersey: Lawrence Erlbaum. Willingham, W. W. & Cole, N. S. (1997). Gender and fair assessment. Mahwah, NJ: Erlbaum. *Wools, S. (2015). All about validity: An evaluation system for the quality of educational assessment. Amsterdam: Author. ISBN: 978-94-6259-709-9 Yalow, E. S., & Popham, W. J. (1983). Content validity at the crossroads. Educational Researcher, 12, 10-14. Ying, L., & Sireci, S. G. (2007). Validity issues in test speededness. Educational Measurement: Issues and Practice, 26(4), 29-37. Zenisky, A. L., Sireci, S. G., Lewis, J., Lim, H., O’Donnell, F., Wells, C. S., Padellaro, F., Jung, H., Banda, E., Pham, D. Hong, S., Park, Y., Botha, S., Lee, M, & Garcia, A. (2018, September). Massachusetts Adult Proficiency Tests for College and Career Readiness: Technical manual. Center for Educational Assessment research report No. 974. Amherst, MA: Center for Educational Assessment. Zumbo, B. D. (2005). Structural equation modeling and test validation. In B. Everitt and D. C. Howell, Encyclopedia of statistics in behavioral science, (pp. 1951-1958). Chichester, UK: Wiley. Zumbo, B. D. (2007). Validity: Foundational Issues and Statistical Methodology. In C.R. Rao and S. Sinharay (Eds.) Handbook of Statistics, Vol. 26: Psychometrics, (pp. 45-79). Elsevier Science B.V.: The Netherlands. Zumbo, B. D. (2009). Validity as contextualized and pragmatic explanation, and its implications for validation practice. History repeats itself again. In R. Lissitz (Ed.), The Concept of Validity: Revisions, New Directions and Applications (pp. 65-81). Charlotte, NC: Information Age Publishing Inc. Zumbo, B. D., & Hubley, A. M. (Eds.) (2017). Understanding and investigating response processes in validation research. Cham, Switzerland: Springer Press. Zumbo, B. D., & Rupp, A. A. (2004). Responsible modeling of measurement data for appropriate inferences: Important advances in reliability and validity theory. In David Kaplan (Ed.), The SAGE Handbook of Quantitative Methodology for the Social Sciences (pp. 73-92). Thousand Oaks, CA: Sage Press. Zwick, R., & Schlemer, L. (2004). SAT validity for linguistic minorities at the University of California, Santa Barbara. Educational Measurement: Issues and Practice, 23(1), 6-16. Validity Syllabus Fall 2022 24 REVISED Class Schedule Fall 2022 Date  Topic  Readings*  9/12  Overview: What we know about validity The 5 Sources of Validity Evidence  Lehman (1999); McGinn (1999); Jenkins (1946); Guilford (1946); Ebel (1961); AERA et al. (2014, pp. 11-22) https://www.youtube.com/watch?v=o o6ebKItW2A  9/19  Validity Past, Present, and Future  Sireci (2009b), Rulon (1946), Cronbach & Meehl (1955)  9/26  Early Conceptualizations of Validity The Construct of Construct Validity  Campbell & Fiske (1959) Pitoniak, Sireci, & Luecht (2002)  10/3  Multitrait-Multimethod Matrix  Messick (1989b, 13-34)  10/8  Unitary Conceptualization of Validity  Messick (1989b, 34-63)  10/10  No Class (Indigenous Peoples Day)  10/17  Unitary Conceptualization of Validity  Messick (1989b, 63-92)  10/24  Consequences, Values, Social Considerations  Shepard (1993)  10/31  Construct Validation Simplified Midterm Due  AERA et al. (2014), pp. 11-31, 183.201; Kane (1992, 2006, 2013); Sireci(2013, 2020a)  11/7  Argument-Based Approach to Validity  Linn (1984); Sireci & Talento-Miller (2006); Zwick & Schlemer (2004)  11/14  Test Validation: Gathering/Analyzing Validity Evidence Based on Relations to Other Variables Final projects must be approved by this date.  Byrne & van der Vijver (2010); Rios & Wells (2014); AERA et. al. (2014), pp. 49-70; 169-182; Sireci & Geisinger(1998); Sireci & Parker (2006)  11/21  Validity Evidence Based on Internal Structure Employment Testing  Lane (2014); Randall (2021b)  11/28  Validity Evidence Based on Testing Consequences Legal and Fairness Issues  Sireci (1998); Sireci & Faulkner-Bond (2014); AERA et. al. (2014), pp. 203.213; Haertel, (2013); Linn (2009); U.S.Dept. of Education (2018)  12/5  Informal Class Discussion (optional)  12/12  Validity Evidence Based on Test ContentValidation Frameworks  12/21  Final Project Due (no class)  Note: Readings will be distributed on the date they are listed and will be discussed the NEXT class. 